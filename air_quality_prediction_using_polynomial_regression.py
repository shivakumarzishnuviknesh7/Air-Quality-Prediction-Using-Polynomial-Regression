# -*- coding: utf-8 -*-
"""Air-Quality-Prediction-Using-Polynomial-Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ti82FfVNRgU1WuRjFA0X2xmT_bf_Seur

# Import Libraries
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

"""# Load the Dataset"""

# Load the dataset
data = pd.read_csv('BeijingPM20100101_20151231.csv')

data

# Inspect the data
print(data.head())

"""# Select Relevant Features and Target Variable"""

# Select relevant features and the target variable
features = ['DEWP', 'TEMP', 'PRES', 'cbwd', 'Iws', 'Is', 'Ir']
target = 'pm2.5'

"""# Handle Missing Values"""

# Handle missing values
data = data.dropna(subset=[target])
X = data[features]
y = data[target]

X

y

"""## Converts categorical variables into dummy/indicator variables (one-hot encoding). The drop_first=True argument avoids multicollinearity by dropping one level of each categorical variable."""

# Convert categorical variables into dummy/indicator variables
X = pd.get_dummies(X, drop_first=True)

"""# Split Data into Training and Testing Sets"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #Splits the data into training (80%) and testing (20%) sets. random_state=42 ensures reproducibility.

"""# Polynomial feature transformation"""

poly = PolynomialFeatures(degree=3)  # Adjust degree as needed
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)
#Transforms the input features into polynomial features of the specified degree. This allows the linear regression model to capture non-linear relationships.

"""# feature scaling"""

# Standardize the features
scaler = StandardScaler()
X_train_poly = scaler.fit_transform(X_train_poly)
X_test_poly = scaler.transform(X_test_poly)

"""# Model Training"""

# Train the polynomial regression model
model = LinearRegression()
model.fit(X_train_poly, y_train)

# Make predictions
y_pred = model.predict(X_test_poly)

# Assuming 'single_data' is your single data point stored in a dictionary
single_data = {
    'DEWP': 10,      # Example value for DEWP feature
    'TEMP': 25,      # Example value for TEMP feature
    'PRES': 1010,    # Example value for PRES feature
    'cbwd': 'NW',    # Example value for cbwd feature
    'Iws': 10,       # Example value for Iws feature
    'Is': 0,         # Example value for Is feature
    'Ir': 0          # Example value for Ir feature
}

# Convert the single data point to DataFrame
single_df = pd.DataFrame(single_data, index=[0])

# Apply the same preprocessing steps as done for the training data
single_df = pd.get_dummies(single_df, drop_first=True)  # Convert categorical variables to dummy variables

# Ensure that all expected dummy variable columns are present
# If any dummy variable columns are missing, add them with values set to 0
expected_columns = set(X.columns)
current_columns = set(single_df.columns)
missing_columns = expected_columns - current_columns
for column in missing_columns:
    single_df[column] = 0

# Reorder the columns to match the order used during training
single_df = single_df[X.columns]

# Apply polynomial feature transformation
single_df_poly = poly.transform(single_df)

# Standardize the features
single_df_poly_scaled = scaler.transform(single_df_poly)

# Make prediction for the single data point
prediction = model.predict(single_df_poly_scaled)

print("Prediction:", prediction[0])

"""# Evaluate the model"""

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')

"""# Visualization"""

plt.scatter(y_test, y_pred, color='blue')
plt.xlabel('Actual PM2.5')
plt.ylabel('Predicted PM2.5')
plt.title('Actual vs Predicted PM2.5')
plt.show()